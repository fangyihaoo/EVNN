{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "fokker4d.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "zF2eXGtspSEz"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from torch import nn, Tensor\n",
        "import torch.nn.init as init\n",
        "from typing import Callable, Tuple\n",
        "from scipy.stats import multivariate_normal\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
        "from scipy.interpolate import griddata\n",
        "import gc\n",
        "\n",
        "_scaling_min = 0.001\n",
        "\n",
        "torch.set_default_dtype(torch.float64)\n",
        "\n",
        "def unsqueeze(x):\n",
        "    return x.unsqueeze(0).unsqueeze(-1).detach()\n",
        "\n",
        "class ActNorm(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    ActNorm layer\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self,\n",
        "                 num_features: int, # number of input dimension\n",
        "                 logscale_factor: float = 1.,\n",
        "                 scale: float = 1.,\n",
        "                 learn_scale: bool = True\n",
        "                 ) -> None:\n",
        "        super(ActNorm, self).__init__()\n",
        "\n",
        "        self.initialized = False\n",
        "        self.num_features = num_features\n",
        "\n",
        "        self.register_parameter('b', nn.Parameter(torch.zeros(1, num_features, 1), requires_grad=True))\n",
        "        self.learn_scale = learn_scale\n",
        "        if learn_scale:\n",
        "            self.logscale_factor = logscale_factor\n",
        "            self.scale = scale\n",
        "            self.register_parameter('logs', nn.Parameter(torch.zeros(1, num_features, 1), requires_grad=True))\n",
        "            \n",
        "    def forward_transform(self, x, logdet=0):\n",
        "        input_shape = x.size()\n",
        "        x = x.view(input_shape[0], input_shape[1], -1)\n",
        "\n",
        "        if not self.initialized:\n",
        "            self.initialized = True\n",
        "            \n",
        "            sum_size = x.size(0) * x.size(-1)\n",
        "            b = -torch.sum(x, dim=(0, -1)) / sum_size\n",
        "            self.b.data.copy_(unsqueeze(b).data)\n",
        "\n",
        "            if self.learn_scale:\n",
        "                var = unsqueeze(torch.sum((x + unsqueeze(b)) ** 2, dim=(0, -1)) / sum_size)\n",
        "                logs = torch.log(self.scale / (torch.sqrt(var) + 1e-6)) / self.logscale_factor\n",
        "                self.logs.data.copy_(logs.data)\n",
        "\n",
        "        b = self.b\n",
        "        output = x + b\n",
        "\n",
        "        if self.learn_scale:\n",
        "            logs = self.logs * self.logscale_factor\n",
        "            scale = torch.exp(logs) + _scaling_min\n",
        "            output = output * scale\n",
        "            dlogdet = torch.sum(torch.log(scale)) * x.size(-1)  # c x h\n",
        "\n",
        "            return output.view(input_shape), logdet + dlogdet\n",
        "        else:\n",
        "            return output.view(input_shape), logdet\n",
        "\n",
        "class ActNormNoLogdet(ActNorm):\n",
        "\n",
        "    def forward(self, x):\n",
        "        return super(ActNormNoLogdet, self).forward_transform(x)[0]\n",
        "\n",
        "class SequentialFlow(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, flows):\n",
        "        super(SequentialFlow, self).__init__()\n",
        "        self.flows = torch.nn.ModuleList(flows)\n",
        "\n",
        "    def forward_transform(self, x, logdet=0):\n",
        "        for flow in self.flows:\n",
        "            x, logdet = flow.forward_transform(x, logdet)\n",
        "        return x, logdet\n",
        "\n",
        "\n",
        "\n",
        "def symm_softplus(x, softplus_=torch.nn.functional.softplus):\n",
        "    return softplus_(x) - 0.5 * x\n",
        "\n",
        "\n",
        "def softplus(x):\n",
        "    return nn.functional.softplus(x)\n",
        "\n",
        "\n",
        "def gaussian_softplus(x):\n",
        "    z = np.sqrt(np.pi / 2)\n",
        "    return (z * x * torch.erf(x / np.sqrt(2)) + torch.exp(-x**2 / 2) + z * x) / (2*z)\n",
        "\n",
        "\n",
        "def gaussian_softplus2(x):\n",
        "    z = np.sqrt(np.pi / 2)\n",
        "    return (z * x * torch.erf(x / np.sqrt(2)) + torch.exp(-x**2 / 2) + z * x) / z\n",
        "\n",
        "\n",
        "def laplace_softplus(x):\n",
        "    return torch.relu(x) + torch.exp(-torch.abs(x)) / 2\n",
        "\n",
        "\n",
        "def cauchy_softplus(x):\n",
        "    # (Pi y + 2 y ArcTan[y] - Log[1 + y ^ 2]) / (2 Pi)\n",
        "    pi = np.pi\n",
        "    return (x * pi - torch.log(x**2 + 1) + 2 * x * torch.atan(x)) / (2*pi)\n",
        "\n",
        "\n",
        "def activation_shifting(activation):\n",
        "    def shifted_activation(x):\n",
        "        return activation(x) - activation(torch.zeros_like(x))\n",
        "    return shifted_activation\n",
        "\n",
        "\n",
        "def get_softplus(softplus_type='softplus', zero_softplus=False):\n",
        "    if softplus_type == 'softplus':\n",
        "        act = nn.functional.softplus\n",
        "    elif softplus_type == 'gaussian_softplus':\n",
        "        act = gaussian_softplus\n",
        "    elif softplus_type == 'gaussian_softplus2':\n",
        "        act = gaussian_softplus2\n",
        "    elif softplus_type == 'laplace_softplus':\n",
        "        act = gaussian_softplus\n",
        "    elif softplus_type == 'cauchy_softplus':\n",
        "        act = cauchy_softplus\n",
        "    else:\n",
        "        raise NotImplementedError(f'softplus type {softplus_type} not supported.')\n",
        "    if zero_softplus:\n",
        "        act = activation_shifting(act)\n",
        "    return act\n",
        "\n",
        "\n",
        "class Softplus(nn.Module):\n",
        "    def __init__(self, softplus_type='softplus', zero_softplus=False):\n",
        "        super(Softplus, self).__init__()\n",
        "        self.softplus_type = softplus_type\n",
        "        self.zero_softplus = zero_softplus\n",
        "\n",
        "    def forward(self, x):\n",
        "        return get_softplus(self.softplus_type, self.zero_softplus)(x)\n",
        "\n",
        "\n",
        "class SymmSoftplus(torch.nn.Module):\n",
        "\n",
        "    def forward(self, x):\n",
        "        return symm_softplus(x)\n",
        "\n",
        "\n",
        "class PosLinear(torch.nn.Linear):\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        gain = 1 / x.size(1)\n",
        "        return nn.functional.linear(x, torch.nn.functional.softplus(self.weight), self.bias) * gain\n",
        "\n",
        "\n",
        "class PosLinear2(torch.nn.Linear):\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        return nn.functional.linear(x, torch.nn.functional.softmax(self.weight, 1), self.bias)\n",
        "\n",
        "\n",
        "class PosConv2d(torch.nn.Conv2d):\n",
        "\n",
        "    def reset_parameters(self) -> None:\n",
        "        super().reset_parameters()\n",
        "        # noinspection PyProtectedMember,PyAttributeOutsideInit\n",
        "        self.fan_in, _ = init._calculate_fan_in_and_fan_out(self.weight)\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        return self._conv_forward(x, torch.nn.functional.softplus(self.weight)) / self.fan_in\n",
        "\n",
        "\n",
        "\n",
        "class ICNN(torch.nn.Module):\n",
        "    def __init__(self, dim=2, dimh=16, num_hidden_layers=1):\n",
        "        super(ICNN, self).__init__()\n",
        "\n",
        "        Wzs = list()\n",
        "        Wzs.append(nn.Linear(dim, dimh))\n",
        "        for _ in range(num_hidden_layers - 1):\n",
        "            Wzs.append(PosLinear(dimh, dimh, bias=False))\n",
        "        Wzs.append(PosLinear(dimh, 1, bias=False))\n",
        "        self.Wzs = torch.nn.ModuleList(Wzs)\n",
        "\n",
        "        Wxs = list()\n",
        "        for _ in range(num_hidden_layers - 1):\n",
        "            Wxs.append(nn.Linear(dim, dimh))\n",
        "        Wxs.append(nn.Linear(dim, 1, bias=False))\n",
        "        self.Wxs = torch.nn.ModuleList(Wxs)\n",
        "        self.act = nn.Softplus()\n",
        "\n",
        "    def forward(self, x):\n",
        "        z = self.act(self.Wzs[0](x))\n",
        "        for Wz, Wx in zip(self.Wzs[1:-1], self.Wxs[:-1]):\n",
        "            z = self.act(Wz(z) + Wx(x))\n",
        "        return self.Wzs[-1](z) + self.Wxs[-1](x)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class ICNN3(torch.nn.Module):\n",
        "    def __init__(self, \n",
        "                 dim: int = 2, # input dimension \n",
        "                 dimh: int = 16, # hidden layer dimension\n",
        "                 num_hidden_layers: int = 2, # number of hidden layer \n",
        "                 symm_act_first: bool = False, # refer to the paper\n",
        "                 softplus_type: str = 'softplus', # refer to the paper\n",
        "                 zero_softplus: bool = False # refer to the paper\n",
        "                 ) -> None:\n",
        "        super(ICNN3, self).__init__()\n",
        "\n",
        "        self.act = Softplus(softplus_type=softplus_type, zero_softplus=zero_softplus)\n",
        "        self.symm_act_first = symm_act_first\n",
        "\n",
        "        Wzs = list()\n",
        "        Wzs.append(nn.Linear(dim, dimh))\n",
        "        for _ in range(num_hidden_layers - 1):\n",
        "            Wzs.append(PosLinear(dimh, dimh // 2, bias=True))\n",
        "        Wzs.append(PosLinear(dimh, 1, bias=False))\n",
        "        self.Wzs = torch.nn.ModuleList(Wzs)\n",
        "\n",
        "        Wxs = list()\n",
        "        for _ in range(num_hidden_layers - 1):\n",
        "            Wxs.append(nn.Linear(dim, dimh // 2))\n",
        "        Wxs.append(nn.Linear(dim, 1, bias=False))\n",
        "        self.Wxs = torch.nn.ModuleList(Wxs)\n",
        "\n",
        "        Wx2s = list()\n",
        "        for _ in range(num_hidden_layers - 1):\n",
        "            Wx2s.append(nn.Linear(dim, dimh // 2))\n",
        "        self.Wx2s = torch.nn.ModuleList(Wx2s)\n",
        "\n",
        "        actnorms = list()\n",
        "        for _ in range(num_hidden_layers - 1):\n",
        "            actnorms.append(ActNormNoLogdet(dimh // 2))\n",
        "        actnorms.append(ActNormNoLogdet(1))\n",
        "        actnorms[-1].b.requires_grad_(False)\n",
        "        self.actnorms = torch.nn.ModuleList(actnorms)\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.symm_act_first:\n",
        "            z = symm_softplus(self.Wzs[0](x), self.act)\n",
        "        else:\n",
        "            z = self.act(self.Wzs[0](x))\n",
        "        for Wz, Wx, Wx2, actnorm in zip(self.Wzs[1:-1], self.Wxs[:-1], self.Wx2s[:], self.actnorms[:-1]):\n",
        "            z = self.act(actnorm(Wz(z) + Wx(x)))\n",
        "            aug = Wx2(x)\n",
        "            aug = symm_softplus(aug, self.act) if self.symm_act_first else self.act(aug)\n",
        "            z = torch.cat([z, aug], 1)\n",
        "        return self.actnorms[-1](self.Wzs[-1](z) + self.Wxs[-1](x))\n",
        "\n",
        "\n",
        "\n",
        "class DeepConvexFlow(torch.nn.Module):\n",
        "    def __init__(self, \n",
        "                 icnn: Callable[..., Tensor], # ICNN object\n",
        "                 bias_w1: float = 0.0, # bias parameters in actnorm layer\n",
        "                 trainable_w0: bool = True # weight parameters in actnorm layer\n",
        "                 ) -> None:\n",
        "        super(DeepConvexFlow, self).__init__()\n",
        "\n",
        "        self.icnn = icnn\n",
        "        self.w0 = torch.nn.Parameter(torch.log(torch.exp(torch.ones(1)) - 1), requires_grad=trainable_w0)\n",
        "        self.w1 = torch.nn.Parameter(torch.zeros(1) + bias_w1)\n",
        "\n",
        "        \n",
        "    def get_potential(self, \n",
        "                      x: Tensor\n",
        "                      ) -> Tensor:\n",
        "        \"\"\"compute the potential F\n",
        "\n",
        "        Args:\n",
        "            x (Tensor): coordinates\n",
        "\n",
        "        Returns:\n",
        "            Tensor: potential F (N, 1)\n",
        "        \"\"\"\n",
        "        n = x.size(0)\n",
        "        icnn = self.icnn(x)\n",
        "        return F.softplus(self.w1) * icnn + F.softplus(self.w0) * (x.view(n, -1) ** 2).sum(1, keepdim=True) / 2\n",
        "    \n",
        "    def forward(self, \n",
        "                x: Tensor\n",
        "                ) -> Tensor:\n",
        "        \"\"\"forward function, compute the convex potential f, gradient of F\n",
        "\n",
        "        Args:\n",
        "            x (Tensor): coordinate\n",
        "\n",
        "        Returns:\n",
        "            Tensor: convex potential, f (N, d)\n",
        "        \"\"\"\n",
        "        with torch.enable_grad():\n",
        "            x = x.clone().requires_grad_(True)\n",
        "            F = self.get_potential(x)\n",
        "            f = torch.autograd.grad(F.sum(), x, create_graph=True)[0]\n",
        "        return f\n",
        "    \n",
        "    def forward_transform(self, \n",
        "                          x: Tensor, \n",
        "                          logdet: Tensor = 0\n",
        "                          ) -> Tuple[Tensor, Tensor]:\n",
        "        \"\"\"compute the convex potential f and log determinant\n",
        "\n",
        "        Args:\n",
        "            x (Tensor): coordinate\n",
        "            logdet (Tensor, optional): log determinant (N, 1). Defaults to 0.\n",
        "\n",
        "        Returns:\n",
        "            Tuple[Tensor, Tensor]: convex potential, log determinant\n",
        "        \"\"\"\n",
        "        \n",
        "        with torch.enable_grad():\n",
        "            x = x.clone().requires_grad_(True)\n",
        "            F = self.get_potential(x)\n",
        "            f = torch.autograd.grad(F.sum(), x, create_graph=True)[0]\n",
        "            H = []\n",
        "            \n",
        "            for i in range(f.shape[1]):\n",
        "                H.append(torch.autograd.grad(f[:, i].sum(), x, create_graph = True, retain_graph = True)[0])\n",
        "                \n",
        "            H = torch.stack(H, dim = 1)\n",
        "        \n",
        "        return f, logdet + torch.slogdet(H).logabsdet\n",
        "\n",
        "\n",
        "\n",
        "def V(phi: Tensor,\n",
        "      mu: Tensor,\n",
        "      sigma: Tensor) -> Tensor:\n",
        "    \"\"\"\n",
        "    Applied bilinear transformation on the each row of the input\n",
        "    V(x) = 1/2 * (x - \\mu)^T \\Sigma^{-1} (x - \\mu)\n",
        "\n",
        "    Args:\n",
        "        phi (Tensor [N, d]): normalizing flow output\n",
        "        mu (Tensor [1, d]): mean vector\n",
        "        sigma (Tensor [d, d]): covariance matrix\n",
        "    \n",
        "    Return:\n",
        "        Tensor [N, 1]\n",
        "    \"\"\"\n",
        "    x_cen = phi - mu\n",
        "    return 0.5 * torch.sum(torch.mm(x_cen, torch.inverse(sigma))*x_cen, 1, keepdim=True)\n",
        "\n",
        "def MulNormal(mu: Tensor,\n",
        "              sigma: Tensor,\n",
        "              pos: Tensor) -> Tensor:\n",
        "    \"\"\"Genrate the density for multivariate gaussian distribution\n",
        "\n",
        "    Args:\n",
        "        mu (Tensor): mean vector\n",
        "        sigma (Tensor): sigma matrix\n",
        "        pos (Tensor): position for density evaluation\n",
        "\n",
        "    Returns:\n",
        "        Density (Tensor): density at position\n",
        "    \"\"\"\n",
        "    rv = multivariate_normal(mu, sigma)\n",
        "    return torch.from_numpy(rv.pdf(pos)).unsqueeze_(1)\n",
        "\n",
        "\n",
        "\n",
        "class OOMFormatter(ticker.ScalarFormatter):\n",
        "    def __init__(self, order=0, fformat=\"%1.1f\", offset=True, mathText=True):\n",
        "        self.oom = order\n",
        "        self.fformat = fformat\n",
        "        ticker.ScalarFormatter.__init__(self,useOffset=offset,useMathText=mathText)\n",
        "    def _set_order_of_magnitude(self):\n",
        "        self.orderOfMagnitude = self.oom\n",
        "    def _set_format(self, vmin=None, vmax=None):\n",
        "        self.format = self.fformat\n",
        "        if self._useMathText:\n",
        "             self.format = r'$\\mathdefault{%s}$' % self.format\n",
        "\n",
        "def DensityPlot(x, y, rho, t, s):\n",
        "    _, ax = plt.subplots(figsize=(10, 10))\n",
        "    h = ax.scatter(x, y, c = rho, alpha=1, cmap= 'viridis',  marker='o', s=10)\n",
        "    ax.grid(color='grey', linestyle='-', linewidth=0.25)\n",
        "    xmin, xmax = np.min(x), np.max(x)\n",
        "    ymin, ymax = np.min(y), np.max(y)\n",
        "    divider = make_axes_locatable(ax)\n",
        "    cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
        "    cb = plt.colorbar(h, cax=cax, format=OOMFormatter(-1, mathText=False)).ax.yaxis.offsetText.set_fontsize(30)\n",
        "    plt.tick_params(labelsize=30)\n",
        "    ax.set_xlim([xmin-0.1, xmax+0.1])\n",
        "    ax.set_ylim([ymin-0.1, ymax+0.1])\n",
        "    ax.tick_params(axis='both', which='major', labelsize=30)\n",
        "    plt.savefig('./' + f'fokker4d' + s + 'phase' + f'{t}.png', pad_inches = 0.05, bbox_inches='tight')\n",
        "    plt.show()\n",
        "    plt.close()\n",
        "    \n",
        "\n",
        "\n",
        "def FokkerPlanck(phi: Tensor,\n",
        "                 coor: Tensor,\n",
        "                 rho: Tensor,\n",
        "                 lgdet: Tensor,\n",
        "                 V:Tensor\n",
        "                 ) -> Tensor:\n",
        "    \"\"\"Loss function for Fokker Planck equation\n",
        "\n",
        "    Args:\n",
        "        phi (Tensor): The convex potential of the normalizing flow [N, d]\n",
        "        coor (Tensor): Input data [N, d]\n",
        "        rho (Tensor): Estimated function [N, 1] \n",
        "        lgdet (Tensor): Log determinant [N, 1]\n",
        "        V (Tensor): density [N, 1]\n",
        "\n",
        "    Returns:\n",
        "        loss\n",
        "    \"\"\"\n",
        "    Cap_Phi = 50*torch.mean(rho*torch.sum((phi - coor)**2, dim = 1, keepdim = True)) # captical phi\n",
        "    vol = torch.mean(rho * (torch.log(rho) - lgdet.unsqueeze_(-1) + V)) # second part\n",
        "    \n",
        "    return Cap_Phi + vol, vol"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dimx = 4\n",
        "depth = 6\n",
        "k = 32\n",
        "symm_act_first = True\n",
        "zero_softplus = False\n",
        "softplus_type = 'gaussian_softplus2'\n",
        "abserror = []\n",
        "relativeerror = []\n",
        "ENERGY = []\n",
        "\n",
        "\n",
        "mu = torch.tensor([1/3, 1/3, 0, 0])\n",
        "# sigma = torch.tensor([[5/8, -3/8, 0, 0], [-3/8, 5/8, 0, 0], [0, 0, 5/8, -3/8], [0, 0, -3/8, 5/8]])\n",
        "sigma = torch.tensor([[5/8, -3/8, 0, 0], [-3/8, 5/8, 0, 0], [0, 0, 1, 0], [0, 0, 0, 1]])\n",
        "\n",
        "\n",
        "mu0 = torch.tensor([0, 0, 0, 0])\n",
        "sigma0 = torch.tensor([[1 , 0, 0, 0], [0, 1, 0, 0], [0, 0, 1, 0], [0, 0, 0, 1]])\n",
        "\n",
        "x, y, z, u = np.random.multivariate_normal(mu0, sigma0, 10000).T\n",
        "x, y, z, u = torch.from_numpy(x).unsqueeze_(1), torch.from_numpy(y).unsqueeze_(1), torch.from_numpy(z).unsqueeze_(1), torch.from_numpy(u).unsqueeze_(1)\n",
        "coor = torch.cat((x, y, z, u), dim = 1)\n",
        "rho = MulNormal(mu0, sigma0, coor)\n",
        "\n",
        "\n",
        "\n",
        "nblocks = 1\n",
        "icnns = [ICNN3(dimx, k, depth, symm_act_first=symm_act_first, softplus_type=softplus_type, zero_softplus=zero_softplus) for _ in range(nblocks)]\n",
        "layers = [None] * (nblocks + 1)\n",
        "layers[0] = ActNorm(dimx)\n",
        "layers[1:] = [DeepConvexFlow(icnn, bias_w1=-0.0, trainable_w0=False) for _, icnn in zip(range(nblocks), icnns)]\n",
        "\n",
        "IDENTICAL = SequentialFlow(layers)\n",
        "flow = SequentialFlow(layers)"
      ],
      "metadata": {
        "id": "rcEI4nhopUmv"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('x1 and x2 phase')\n",
        "DensityPlot(coor.numpy()[:,0], coor.numpy()[:,1], rho, 0, 'x1x2')\n",
        "print('x3 and x4 phase')\n",
        "DensityPlot(coor.numpy()[:,2], coor.numpy()[:,3], rho, 0, 'x3x4')"
      ],
      "metadata": {
        "id": "fBNQJc83f3oy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def Identity(phi, coor):\n",
        "    return torch.mean(torch.sum((phi - coor)**2, dim = 1, keepdim = True))\n",
        "\n",
        "def FokkerPretrain(model, coor):\n",
        "    # x = torch.linspace(-3, 3, 10)\n",
        "    # y = torch.linspace(-3, 3, 10)\n",
        "    # u = torch.linspace(-3, 3, 10)\n",
        "    # z = torch.linspace(-3, 3, 10)\n",
        "    # X, Y, U, Z = torch.meshgrid(x, y, u, z)\n",
        "    # coor = torch.cat((X.flatten()[:, None], Y.flatten()[:, None], U.flatten()[:, None], Z.flatten()[:, None]), dim=1)\n",
        "\n",
        "    optimizer = torch.optim.LBFGS(model.parameters(),\n",
        "                            history_size = 50,\n",
        "                            max_iter = 50,\n",
        "                            line_search_fn= 'strong_wolfe')\n",
        "    def closure():\n",
        "        optimizer.zero_grad()\n",
        "        phi, lgdet = model.forward_transform(coor)\n",
        "        loss = Identity(phi, coor)\n",
        "        loss.backward()\n",
        "        return loss\n",
        "\n",
        "    optimizer.step(closure)\n",
        "\n",
        "\n",
        "FokkerPretrain(IDENTICAL, coor)\n",
        "\n",
        "# gc.collect()\n",
        "\n",
        "new, lgd = IDENTICAL.forward_transform(coor)\n",
        "flow.load_state_dict(IDENTICAL.state_dict())\n",
        "print(Identity(new, coor))\n"
      ],
      "metadata": {
        "id": "nZeV6yCXtmY9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6c174aca-a797-44da-e2dc-218d499c28e4"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(6.1726e-06, grad_fn=<MeanBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(new)\n",
        "print('==========')\n",
        "print(coor)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kqz9TxJVuUZp",
        "outputId": "2fea1d81-c224-4398-a740-f97e47b8aef5"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-0.7311,  0.1254, -0.1722,  0.7502],\n",
            "        [-0.0136, -0.4161,  1.2133,  1.8655],\n",
            "        [ 0.2221, -1.3303, -0.0870,  1.4748],\n",
            "        ...,\n",
            "        [-0.6003, -1.2963,  0.3312, -0.3242],\n",
            "        [-0.5049, -0.9717,  2.5736,  0.4185],\n",
            "        [ 0.6274,  0.8074,  0.7387,  0.6714]], grad_fn=<AddBackward0>)\n",
            "==========\n",
            "tensor([[-0.7306,  0.1245, -0.1715,  0.7506],\n",
            "        [-0.0132, -0.4161,  1.2134,  1.8642],\n",
            "        [ 0.2214, -1.3297, -0.0870,  1.4738],\n",
            "        ...,\n",
            "        [-0.5990, -1.2949,  0.3308, -0.3222],\n",
            "        [-0.5039, -0.9709,  2.5712,  0.4198],\n",
            "        [ 0.6273,  0.8063,  0.7396,  0.6705]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "for epoch in range(100):\n",
        "    t = 0.01*(epoch + 1)\n",
        "    shift = (1 - math.exp(-4*t))\n",
        "    mt = torch.tensor([1/3 * shift, 1/3 * shift, 0, 0])\n",
        "    # sigt = torch.tensor([[5/8 + 3/8 * math.exp(-8*t), -3/8 + 3/8 * math.exp(-8*t), 0, 0], [-3/8 + 3/8 * math.exp(-8*t), 5/8 + 3/8 * math.exp(-8*t), 0, 0], [0, 0, 5/8 + 3/8 * math.exp(-8*t), -3/8 + 3/8 * math.exp(-8*t)], [0, 0, -3/8 + 3/8 * math.exp(-8*t), 5/8 + 3/8 * math.exp(-8*t)]])\n",
        "    sigt = torch.tensor([[5/8 + 3/8 * math.exp(-8*t), -3/8 + 3/8 * math.exp(-8*t), 0, 0], [-3/8 + 3/8 * math.exp(-8*t), 5/8 + 3/8 * math.exp(-8*t), 0, 0], [0, 0, 1, 0], [0, 0, 0, 1]])\n",
        "    \n",
        "    # print(epoch)\n",
        "    optimizer = torch.optim.LBFGS(flow.parameters(),\n",
        "                            history_size=50,\n",
        "                            max_iter = 50,\n",
        "                            line_search_fn= 'strong_wolfe')\n",
        "    def closure():\n",
        "        optimizer.zero_grad()\n",
        "        phi, lgdet = flow.forward_transform(coor)\n",
        "        loss = FokkerPlanck(phi, coor, rho, lgdet, V(phi, mu, sigma))[0]\n",
        "        loss.backward()\n",
        "        return loss\n",
        "\n",
        "    optimizer.step(closure)\n",
        "\n",
        "    phi, lgdet = flow.forward_transform(coor)\n",
        "    with torch.no_grad():\n",
        "        energy = FokkerPlanck(phi, coor, rho, lgdet, V(phi, mu, sigma))[1]\n",
        "        ENERGY.append(energy.item())\n",
        "\n",
        "    coor = phi.clone().detach()\n",
        "    # coor = coor.detach()\n",
        "    # lgdet = lgdet.unsqueeze_(-1).detach()\n",
        "    lgdet = lgdet.detach()\n",
        "    rho = rho / (torch.exp(lgdet) + 1e-8)\n",
        "\n",
        "\n",
        "    act_val = MulNormal(mt, sigt, coor)\n",
        "    aberr = torch.mean((rho - act_val)**2)\n",
        "    abserror.append(aberr.item())\n",
        "    err = torch.mean((rho - act_val)**2)/torch.mean(act_val**2)\n",
        "    relativeerror.append(err.item())\n",
        "\n",
        "    print(f'in epoch {epoch}, the L2 relative error is {err.item()}')\n",
        "    print('x1 and x2 phase')\n",
        "    DensityPlot(coor.numpy()[:,0], coor.numpy()[:,1], rho, epoch+1, 'x1x2')\n",
        "    print('x3 and x4 phase')\n",
        "    DensityPlot(coor.numpy()[:,2], coor.numpy()[:,3], rho, epoch+1, 'x3x4')\n",
        "    torch.save(coor, f'./fokker4dcoor{epoch+1}.pt')\n",
        "    torch.save(rho, f'./fokker4drho{epoch+1}.pt')\n",
        "    flow.load_state_dict(IDENTICAL.state_dict())\n"
      ],
      "metadata": {
        "id": "TLIh37PVzl3L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.mean(act_val**2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QiZCvymKXw8g",
        "outputId": "a90677d9-4bfb-4349-ae6d-2f3dd88b8f8e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.0001)"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "fig, ax = plt.subplots(1, 1, figsize=(8, 5))\n",
        "# ax.set_title(r'$f = 2\\sin{x}\\cdot\\cos{y}$')\n",
        "interval = list(range(1,101))\n",
        "interval = [0.01*ele for ele in interval]\n",
        "# val = [abs(ele) for ele in ENERGY]\n",
        "# lines = []\n",
        "ax.set_yscale('log')\n",
        "lines = ax.plot(interval, relativeerror,  color= '#EE82EE' )\n",
        "ax.grid(color='grey', linestyle='-', linewidth=0.25, alpha=0.5)\n",
        "ax.set_xlabel('time', fontsize = 20)\n",
        "ax.set_ylabel('L2 Relative Eroor', fontsize = 20)\n",
        "ax.tick_params(axis='both', which='major', labelsize=20)\n",
        "plt.xlim([-0.01, 1])\n",
        "plt.savefig('./Fokker4dL2Relative.png',pad_inches = 0.05, bbox_inches='tight')\n",
        "plt.show()\n",
        "plt.close()"
      ],
      "metadata": {
        "id": "8RAPSf1z0ePA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 352
        },
        "outputId": "4876a6c8-5ec9-4c47-9b56-046a406e69cf"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 576x360 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(abserror, './fokker4dabserr.pt')\n",
        "torch.save(relativeerror, './fokker4drelerr.pt')"
      ],
      "metadata": {
        "id": "4YzDD1PQAU1q"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r /content/file.zip /content/fokker4d"
      ],
      "metadata": {
        "id": "HUM6sbGfbLEy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download(\"/content/file.zip\")"
      ],
      "metadata": {
        "id": "V7qQ4QaicR4T",
        "outputId": "1fa03ea6-651f-4778-cff2-b12bd7d25615",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_619c92e1-e69c-41ce-87cc-544209c2d459\", \"file.zip\", 92774639)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}