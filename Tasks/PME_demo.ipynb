{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled27.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "OeIoGAneMuYK"
      },
      "outputs": [],
      "source": [
        "\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from torch import nn, Tensor\n",
        "import torch.nn.init as init\n",
        "from typing import Callable, Tuple\n",
        "from scipy.stats import multivariate_normal\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
        "from scipy.interpolate import griddata\n",
        "import math\n",
        "\n",
        "_scaling_min = 0.001\n",
        "\n",
        "torch.set_default_dtype(torch.float64)\n",
        "\n",
        "def unsqueeze(x):\n",
        "    return x.unsqueeze(0).unsqueeze(-1).detach()\n",
        "\n",
        "class ActNorm(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    ActNorm layer\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self,\n",
        "                 num_features: int, # number of input dimension\n",
        "                 logscale_factor: float = 1.,\n",
        "                 scale: float = 1.,\n",
        "                 learn_scale: bool = True\n",
        "                 ) -> None:\n",
        "        super(ActNorm, self).__init__()\n",
        "\n",
        "        self.initialized = False\n",
        "        self.num_features = num_features\n",
        "\n",
        "        self.register_parameter('b', nn.Parameter(torch.zeros(1, num_features, 1), requires_grad=True))\n",
        "        self.learn_scale = learn_scale\n",
        "        if learn_scale:\n",
        "            self.logscale_factor = logscale_factor\n",
        "            self.scale = scale\n",
        "            self.register_parameter('logs', nn.Parameter(torch.zeros(1, num_features, 1), requires_grad=True))\n",
        "            \n",
        "    def forward_transform(self, x, logdet=0):\n",
        "        input_shape = x.size()\n",
        "        x = x.view(input_shape[0], input_shape[1], -1)\n",
        "\n",
        "        if not self.initialized:\n",
        "            self.initialized = True\n",
        "            \n",
        "            sum_size = x.size(0) * x.size(-1)\n",
        "            b = -torch.sum(x, dim=(0, -1)) / sum_size\n",
        "            self.b.data.copy_(unsqueeze(b).data)\n",
        "\n",
        "            if self.learn_scale:\n",
        "                var = unsqueeze(torch.sum((x + unsqueeze(b)) ** 2, dim=(0, -1)) / sum_size)\n",
        "                logs = torch.log(self.scale / (torch.sqrt(var) + 1e-6)) / self.logscale_factor\n",
        "                self.logs.data.copy_(logs.data)\n",
        "\n",
        "        b = self.b\n",
        "        output = x + b\n",
        "\n",
        "        if self.learn_scale:\n",
        "            logs = self.logs * self.logscale_factor\n",
        "            scale = torch.exp(logs) + _scaling_min\n",
        "            output = output * scale\n",
        "            dlogdet = torch.sum(torch.log(scale)) * x.size(-1)  # c x h\n",
        "\n",
        "            return output.view(input_shape), logdet + dlogdet\n",
        "        else:\n",
        "            return output.view(input_shape), logdet\n",
        "\n",
        "class ActNormNoLogdet(ActNorm):\n",
        "\n",
        "    def forward(self, x):\n",
        "        return super(ActNormNoLogdet, self).forward_transform(x)[0]\n",
        "\n",
        "class SequentialFlow(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, flows):\n",
        "        super(SequentialFlow, self).__init__()\n",
        "        self.flows = torch.nn.ModuleList(flows)\n",
        "\n",
        "    def forward_transform(self, x, logdet=0):\n",
        "        for flow in self.flows:\n",
        "            x, logdet = flow.forward_transform(x, logdet)\n",
        "        return x, logdet\n",
        "\n",
        "\n",
        "\n",
        "def symm_softplus(x, softplus_=torch.nn.functional.softplus):\n",
        "    return softplus_(x) - 0.5 * x\n",
        "\n",
        "\n",
        "def softplus(x):\n",
        "    return nn.functional.softplus(x)\n",
        "\n",
        "\n",
        "def gaussian_softplus(x):\n",
        "    z = np.sqrt(np.pi / 2)\n",
        "    return (z * x * torch.erf(x / np.sqrt(2)) + torch.exp(-x**2 / 2) + z * x) / (2*z)\n",
        "\n",
        "\n",
        "def gaussian_softplus2(x):\n",
        "    z = np.sqrt(np.pi / 2)\n",
        "    return (z * x * torch.erf(x / np.sqrt(2)) + torch.exp(-x**2 / 2) + z * x) / z\n",
        "\n",
        "\n",
        "def laplace_softplus(x):\n",
        "    return torch.relu(x) + torch.exp(-torch.abs(x)) / 2\n",
        "\n",
        "\n",
        "def cauchy_softplus(x):\n",
        "    # (Pi y + 2 y ArcTan[y] - Log[1 + y ^ 2]) / (2 Pi)\n",
        "    pi = np.pi\n",
        "    return (x * pi - torch.log(x**2 + 1) + 2 * x * torch.atan(x)) / (2*pi)\n",
        "\n",
        "\n",
        "def activation_shifting(activation):\n",
        "    def shifted_activation(x):\n",
        "        return activation(x) - activation(torch.zeros_like(x))\n",
        "    return shifted_activation\n",
        "\n",
        "\n",
        "def get_softplus(softplus_type='softplus', zero_softplus=False):\n",
        "    if softplus_type == 'softplus':\n",
        "        act = nn.functional.softplus\n",
        "    elif softplus_type == 'gaussian_softplus':\n",
        "        act = gaussian_softplus\n",
        "    elif softplus_type == 'gaussian_softplus2':\n",
        "        act = gaussian_softplus2\n",
        "    elif softplus_type == 'laplace_softplus':\n",
        "        act = gaussian_softplus\n",
        "    elif softplus_type == 'cauchy_softplus':\n",
        "        act = cauchy_softplus\n",
        "    else:\n",
        "        raise NotImplementedError(f'softplus type {softplus_type} not supported.')\n",
        "    if zero_softplus:\n",
        "        act = activation_shifting(act)\n",
        "    return act\n",
        "\n",
        "\n",
        "class Softplus(nn.Module):\n",
        "    def __init__(self, softplus_type='softplus', zero_softplus=False):\n",
        "        super(Softplus, self).__init__()\n",
        "        self.softplus_type = softplus_type\n",
        "        self.zero_softplus = zero_softplus\n",
        "\n",
        "    def forward(self, x):\n",
        "        return get_softplus(self.softplus_type, self.zero_softplus)(x)\n",
        "\n",
        "\n",
        "class SymmSoftplus(torch.nn.Module):\n",
        "\n",
        "    def forward(self, x):\n",
        "        return symm_softplus(x)\n",
        "\n",
        "\n",
        "class PosLinear(torch.nn.Linear):\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        gain = 1 / x.size(1)\n",
        "        return nn.functional.linear(x, torch.nn.functional.softplus(self.weight), self.bias) * gain\n",
        "\n",
        "\n",
        "class PosLinear2(torch.nn.Linear):\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        return nn.functional.linear(x, torch.nn.functional.softmax(self.weight, 1), self.bias)\n",
        "\n",
        "\n",
        "class PosConv2d(torch.nn.Conv2d):\n",
        "\n",
        "    def reset_parameters(self) -> None:\n",
        "        super().reset_parameters()\n",
        "        # noinspection PyProtectedMember,PyAttributeOutsideInit\n",
        "        self.fan_in, _ = init._calculate_fan_in_and_fan_out(self.weight)\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        return self._conv_forward(x, torch.nn.functional.softplus(self.weight)) / self.fan_in\n",
        "\n",
        "\n",
        "\n",
        "class ICNN(torch.nn.Module):\n",
        "    def __init__(self, dim=2, dimh=16, num_hidden_layers=1):\n",
        "        super(ICNN, self).__init__()\n",
        "\n",
        "        Wzs = list()\n",
        "        Wzs.append(nn.Linear(dim, dimh))\n",
        "        for _ in range(num_hidden_layers - 1):\n",
        "            Wzs.append(PosLinear(dimh, dimh, bias=False))\n",
        "        Wzs.append(PosLinear(dimh, 1, bias=False))\n",
        "        self.Wzs = torch.nn.ModuleList(Wzs)\n",
        "\n",
        "        Wxs = list()\n",
        "        for _ in range(num_hidden_layers - 1):\n",
        "            Wxs.append(nn.Linear(dim, dimh))\n",
        "        Wxs.append(nn.Linear(dim, 1, bias=False))\n",
        "        self.Wxs = torch.nn.ModuleList(Wxs)\n",
        "        self.act = nn.Softplus()\n",
        "\n",
        "    def forward(self, x):\n",
        "        z = self.act(self.Wzs[0](x))\n",
        "        for Wz, Wx in zip(self.Wzs[1:-1], self.Wxs[:-1]):\n",
        "            z = self.act(Wz(z) + Wx(x))\n",
        "        return self.Wzs[-1](z) + self.Wxs[-1](x)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class ICNN3(torch.nn.Module):\n",
        "    def __init__(self, \n",
        "                 dim: int = 2, # input dimension \n",
        "                 dimh: int = 16, # hidden layer dimension\n",
        "                 num_hidden_layers: int = 2, # number of hidden layer \n",
        "                 symm_act_first: bool = False, # refer to the paper\n",
        "                 softplus_type: str = 'softplus', # refer to the paper\n",
        "                 zero_softplus: bool = False # refer to the paper\n",
        "                 ) -> None:\n",
        "        super(ICNN3, self).__init__()\n",
        "\n",
        "        self.act = Softplus(softplus_type=softplus_type, zero_softplus=zero_softplus)\n",
        "        self.symm_act_first = symm_act_first\n",
        "\n",
        "        Wzs = list()\n",
        "        Wzs.append(nn.Linear(dim, dimh))\n",
        "        for _ in range(num_hidden_layers - 1):\n",
        "            Wzs.append(PosLinear(dimh, dimh // 2, bias=True))\n",
        "        Wzs.append(PosLinear(dimh, 1, bias=False))\n",
        "        self.Wzs = torch.nn.ModuleList(Wzs)\n",
        "\n",
        "        Wxs = list()\n",
        "        for _ in range(num_hidden_layers - 1):\n",
        "            Wxs.append(nn.Linear(dim, dimh // 2))\n",
        "        Wxs.append(nn.Linear(dim, 1, bias=False))\n",
        "        self.Wxs = torch.nn.ModuleList(Wxs)\n",
        "\n",
        "        Wx2s = list()\n",
        "        for _ in range(num_hidden_layers - 1):\n",
        "            Wx2s.append(nn.Linear(dim, dimh // 2))\n",
        "        self.Wx2s = torch.nn.ModuleList(Wx2s)\n",
        "\n",
        "        actnorms = list()\n",
        "        for _ in range(num_hidden_layers - 1):\n",
        "            actnorms.append(ActNormNoLogdet(dimh // 2))\n",
        "        actnorms.append(ActNormNoLogdet(1))\n",
        "        actnorms[-1].b.requires_grad_(False)\n",
        "        self.actnorms = torch.nn.ModuleList(actnorms)\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.symm_act_first:\n",
        "            z = symm_softplus(self.Wzs[0](x), self.act)\n",
        "        else:\n",
        "            z = self.act(self.Wzs[0](x))\n",
        "        for Wz, Wx, Wx2, actnorm in zip(self.Wzs[1:-1], self.Wxs[:-1], self.Wx2s[:], self.actnorms[:-1]):\n",
        "            z = self.act(actnorm(Wz(z) + Wx(x)))\n",
        "            aug = Wx2(x)\n",
        "            aug = symm_softplus(aug, self.act) if self.symm_act_first else self.act(aug)\n",
        "            z = torch.cat([z, aug], 1)\n",
        "        return self.actnorms[-1](self.Wzs[-1](z) + self.Wxs[-1](x))\n",
        "\n",
        "\n",
        "\n",
        "class DeepConvexFlow(torch.nn.Module):\n",
        "    def __init__(self, \n",
        "                 icnn: Callable[..., Tensor], # ICNN object\n",
        "                 bias_w1: float = 0.0, # bias parameters in actnorm layer\n",
        "                 trainable_w0: bool = True # weight parameters in actnorm layer\n",
        "                 ) -> None:\n",
        "        super(DeepConvexFlow, self).__init__()\n",
        "\n",
        "        self.icnn = icnn\n",
        "        self.w0 = torch.nn.Parameter(torch.log(torch.exp(torch.ones(1)) - 1), requires_grad=trainable_w0)\n",
        "        self.w1 = torch.nn.Parameter(torch.zeros(1) + bias_w1)\n",
        "\n",
        "        \n",
        "    def get_potential(self, \n",
        "                      x: Tensor\n",
        "                      ) -> Tensor:\n",
        "        \"\"\"compute the potential F\n",
        "\n",
        "        Args:\n",
        "            x (Tensor): coordinates\n",
        "\n",
        "        Returns:\n",
        "            Tensor: potential F (N, 1)\n",
        "        \"\"\"\n",
        "        n = x.size(0)\n",
        "        icnn = self.icnn(x)\n",
        "        return F.softplus(self.w1) * icnn + F.softplus(self.w0) * (x.view(n, -1) ** 2).sum(1, keepdim=True) / 2\n",
        "    \n",
        "    def forward(self, \n",
        "                x: Tensor\n",
        "                ) -> Tensor:\n",
        "        \"\"\"forward function, compute the convex potential f, gradient of F\n",
        "\n",
        "        Args:\n",
        "            x (Tensor): coordinate\n",
        "\n",
        "        Returns:\n",
        "            Tensor: convex potential, f (N, d)\n",
        "        \"\"\"\n",
        "        with torch.enable_grad():\n",
        "            x = x.clone().requires_grad_(True)\n",
        "            F = self.get_potential(x)\n",
        "            f = torch.autograd.grad(F.sum(), x, create_graph=True)[0]\n",
        "        return f\n",
        "    \n",
        "    def forward_transform(self, \n",
        "                          x: Tensor, \n",
        "                          logdet: Tensor = 0\n",
        "                          ) -> Tuple[Tensor, Tensor]:\n",
        "        \"\"\"compute the convex potential f and log determinant\n",
        "\n",
        "        Args:\n",
        "            x (Tensor): coordinate\n",
        "            logdet (Tensor, optional): log determinant (N, 1). Defaults to 0.\n",
        "\n",
        "        Returns:\n",
        "            Tuple[Tensor, Tensor]: convex potential, log determinant\n",
        "        \"\"\"\n",
        "        \n",
        "        with torch.enable_grad():\n",
        "            x = x.clone().requires_grad_(True)\n",
        "            F = self.get_potential(x)\n",
        "            f = torch.autograd.grad(F.sum(), x, create_graph=True)[0]\n",
        "            H = []\n",
        "            \n",
        "            for i in range(f.shape[1]):\n",
        "                H.append(torch.autograd.grad(f[:, i].sum(), x, create_graph = True, retain_graph = True)[0])\n",
        "                \n",
        "            H = torch.stack(H, dim = 1)\n",
        "        \n",
        "        return f, logdet + torch.slogdet(H).logabsdet\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class OOMFormatter(ticker.ScalarFormatter):\n",
        "    def __init__(self, order=0, fformat=\"%1.1f\", offset=True, mathText=True):\n",
        "        self.oom = order\n",
        "        self.fformat = fformat\n",
        "        ticker.ScalarFormatter.__init__(self,useOffset=offset,useMathText=mathText)\n",
        "    def _set_order_of_magnitude(self):\n",
        "        self.orderOfMagnitude = self.oom\n",
        "    def _set_format(self, vmin=None, vmax=None):\n",
        "        self.format = self.fformat\n",
        "        if self._useMathText:\n",
        "             self.format = r'$\\mathdefault{%s}$' % self.format\n",
        "\n",
        "def DensityPlot(x, y, rho, t):\n",
        "    _, ax = plt.subplots(figsize=(10, 10))\n",
        "    h = ax.scatter(x, y, c = rho, alpha=1, cmap= 'viridis',  marker='o', s=10)\n",
        "    ax.grid(color='grey', linestyle='-', linewidth=0.3)\n",
        "    divider = make_axes_locatable(ax)\n",
        "    cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
        "    plt.colorbar(h, cax=cax, format=OOMFormatter(-1, mathText=False)).ax.yaxis.offsetText.set_fontsize(30)\n",
        "    plt.tick_params(labelsize=30)\n",
        "    ax.set_xlim([-1.5, 1.5])\n",
        "    ax.set_ylim([-1.5, 1.5])\n",
        "    ax.tick_params(axis='both', which='major', labelsize=30)\n",
        "    plt.savefig(f'./plots/porus2d{t}.png', pad_inches = 0.05, bbox_inches='tight')\n",
        "    plt.show()\n",
        "    plt.close()\n",
        "\n",
        "def Porus(phi: Tensor,\n",
        "                 coor: Tensor,\n",
        "                 rho: Tensor,\n",
        "                 lgdet: Tensor,\n",
        "                 alpha: float\n",
        "                 ) -> Tensor:\n",
        "    \"\"\"Loss function for Fokker Planck equation\n",
        "\n",
        "    Args:\n",
        "        phi (Tensor): The convex potential of the normalizing flow [N, d]\n",
        "        coor (Tensor): Input data [N, d]\n",
        "        rho (Tensor): Estimated function [N, 1] \n",
        "        lgdet (Tensor): Log determinant [1, N]\n",
        "        V (Tensor): density [N, 1]\n",
        "\n",
        "    Returns:\n",
        "        loss\n",
        "    \"\"\"\n",
        "    Cap_Phi = 100 * torch.mean(rho*torch.sum((phi - coor)**2, dim = 1, keepdim = True)) # captical phi\n",
        "    vol = torch.mean(1/(alpha - 1) * (rho / (torch.exp(lgdet)+1e-8))**(alpha) * torch.exp(lgdet)) # second part\n",
        "    \n",
        "    return Cap_Phi + vol, vol\n",
        "\n",
        "def Barenblatt(x: Tensor, \n",
        "        t: float\n",
        "        ) -> Tensor:\n",
        "\n",
        "    # print(x)\n",
        "\n",
        "    c0 = 0.1\n",
        "    alpha = 4\n",
        "    d = x.size()[1]\n",
        "    # print(t)\n",
        "    k = 1/(alpha - 1 + 2/d)\n",
        "\n",
        "    # print(k)\n",
        "\n",
        "    val = c0 - k * (alpha - 1) / (t**(2*k/d) * (2 * d * alpha)) * torch.sum(x**2, dim = 1, keepdim = True) \n",
        "    return F.relu(val, inplace=True)**(1 / (alpha - 1)) * t**(-k)\n",
        "\n",
        "def disk_grid_regular (n, r, c, ng ):\n",
        "    \"copy from https://people.sc.fsu.edu/~jburkardt/py_src/disk_grid/disk_grid.py\"\n",
        "\n",
        "    cg = np.zeros ( [ 2, ng ] )\n",
        "\n",
        "    p = 0\n",
        "\n",
        "    for j in range ( 0, n + 2 ):\n",
        "\n",
        "        i = 0\n",
        "        x = c[0]\n",
        "        y = c[1] + r * float ( 2 * j ) / float ( 2 * n + 1 )\n",
        "        cg[0,p] = x\n",
        "        cg[1,p] = y\n",
        "        p = p + 1\n",
        "\n",
        "        if ( 0 < j ):\n",
        "\n",
        "            cg[0,p] = x\n",
        "            cg[1,p] = 2.0 * c[1] - y\n",
        "            p = p + 1\n",
        "\n",
        "        while ( True ):\n",
        "\n",
        "            i = i + 1\n",
        "            x = c[0] + r * float ( 2 * i ) / float ( 2 * n + 1 )\n",
        "            if ( r * r < ( x - c[0] ) ** 2 + ( y - c[1] ) ** 2 ):\n",
        "                break\n",
        "\n",
        "            cg[0,p] = x\n",
        "            cg[1,p] = y\n",
        "            p = p + 1\n",
        "            cg[0,p] = 2.0 * c[0] - x\n",
        "            cg[1,p] = y\n",
        "            p = p + 1\n",
        "\n",
        "            if ( 0 < j ):\n",
        "                cg[0,p] = x\n",
        "                cg[1,p] = 2.0 * c[1] - y\n",
        "                p = p + 1;\n",
        "                cg[0,p] = 2.0 * c[0] - x\n",
        "                cg[1,p] = 2.0 * c[1] - y\n",
        "                p = p + 1\n",
        "    cg = np.transpose(cg)\n",
        "    return cg[~np.all(cg == 0, axis=1)]\n",
        "\n",
        "def disk_grid_regular_count ( n, r, c ):\n",
        "\n",
        "#*****************************************************************************80\n",
        "#\n",
        "## disk_grid_regular_count() counts the grid points inside a disk.\n",
        "#\n",
        "#  Discussion:\n",
        "#\n",
        "#    The grid is defined by specifying the radius and center of the disk,\n",
        "#    and the number of subintervals N into which the horizontal radius\n",
        "#    should be divided.  Thus, a value of N = 2 will result in 5 points\n",
        "#    along that horizontal line.\n",
        "#\n",
        "#  Licensing:\n",
        "#\n",
        "#    This code is distributed under the GNU LGPL license.\n",
        "#\n",
        "#  Modified:\n",
        "#\n",
        "#    07 April 2015\n",
        "#\n",
        "#  Author:\n",
        "#\n",
        "#    John Burkardt\n",
        "#\n",
        "#  Input:\n",
        "#\n",
        "#    integer N, the number of subintervals.\n",
        "#\n",
        "#    real R, the radius of the disk.\n",
        "#\n",
        "#    real C(2), the coordinates of the center of the disk.\n",
        "#\n",
        "#  Output:\n",
        "#\n",
        "#    integer NG, the number of grid points inside the disk.\n",
        "#\n",
        "    ng = 0\n",
        "\n",
        "    for j in range ( 0, n + 2 ):\n",
        "\n",
        "        i = 0\n",
        "        x = c[0]\n",
        "        y = c[1] + r * float ( 2 * j ) / float ( 2 * n + 1 )\n",
        "        ng = ng + 1\n",
        "        if ( 0 < j ):\n",
        "            ng = ng + 1\n",
        "\n",
        "        while ( True ):\n",
        "\n",
        "            i = i + 1\n",
        "            x = c[0] + r * float ( 2 * i ) / float ( 2 * n + 1 )\n",
        "\n",
        "            if ( r * r < ( x - c[0] ) ** 2 + ( y - c[1] ) ** 2 ):\n",
        "                break\n",
        "\n",
        "            ng = ng + 1\n",
        "            ng = ng + 1\n",
        "\n",
        "            if ( 0 < j ):\n",
        "                ng = ng + 1\n",
        "                ng = ng + 1\n",
        "    return ng"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "center = torch.tensor([[0.,0.]])\n",
        "c0 = 0.1\n",
        "alpha = 4\n",
        "d = 2\n",
        "kk = 1/(alpha - 1 + 2/d)\n",
        "print(kk)\n",
        "r = 0.1**(kk/d) * math.sqrt(2*d*alpha*c0/(kk*(alpha - 1)))\n",
        "print(r)\n",
        "ng = disk_grid_regular_count(50, r - 0.01, (0,0))\n",
        "coor = disk_grid_regular(50, r - 0.01, (0, 0), ng)\n",
        "coor = torch.tensor(coor)\n",
        "coor = torch.cat((coor, center), dim=0)\n",
        "\n",
        "theta = torch.linspace(0, 2 * torch.pi, 501)\n",
        "\n",
        "for k in range(1):\n",
        "  x = (r - k * 0.01) * torch.cos(theta)\n",
        "  y = (r - k * 0.01) * torch.sin(theta)\n",
        "  coor1 = torch.cat( (x.flatten()[:, None], y.flatten()[:, None]), dim = 1 )\n",
        "  coor = torch.cat((coor, coor1), dim = 0)\n",
        "\n",
        "coor.size()\n",
        "\n",
        "rho = Barenblatt(coor, 0.1)\n",
        "DensityPlot(coor.numpy()[:,0], coor.numpy()[:,1], rho.numpy(), 0)\n",
        "\n",
        "dimx = 2\n",
        "nblocks = 1\n",
        "depth = 6\n",
        "k = 32\n",
        "symm_act_first = True\n",
        "zero_softplus = False\n",
        "softplus_type = 'gaussian_softplus2'\n",
        "\n",
        "\n",
        "\n",
        "icnns = [ICNN3(dimx, k, depth, symm_act_first=symm_act_first, softplus_type=softplus_type, zero_softplus=zero_softplus) for _ in range(nblocks)]\n",
        "# icnns = [ICNN(dimx, k, depth) for _ in range(nblocks)]\n",
        "layers = [None] * (nblocks + 1)\n",
        "layers[0] = ActNorm(dimx)\n",
        "layers[1:] = [DeepConvexFlow(icnn, bias_w1=-0.0, trainable_w0=False) for _, icnn in zip(range(nblocks), icnns)]\n",
        "\n",
        "IDENTICAL = SequentialFlow(layers)\n",
        "flow = SequentialFlow(layers)"
      ],
      "metadata": {
        "id": "GRpPqgXhM_ON"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def Identity(phi, coor):\n",
        "    return torch.mean(torch.sum((phi - coor)**2, dim = 1, keepdim = True))\n",
        "\n",
        "def PorusPretrain(model):\n",
        "    x = torch.linspace(-3, 3, 101)\n",
        "    y = torch.linspace(-3, 3, 101)\n",
        "    X, Y = torch.meshgrid(x, y)\n",
        "    coor = torch.cat((X.flatten()[:, None], Y.flatten()[:, None]), dim=1)\n",
        "\n",
        "    optimizer = torch.optim.LBFGS(model.parameters(),\n",
        "                            history_size=100,\n",
        "                            max_iter = 100,\n",
        "                            line_search_fn= 'strong_wolfe')\n",
        "    def closure():\n",
        "        optimizer.zero_grad()\n",
        "        phi, lgdet = model.forward_transform(coor)\n",
        "        loss = Identity(phi, coor)\n",
        "        loss.backward()\n",
        "        return loss\n",
        "\n",
        "    optimizer.step(closure)\n",
        "\n",
        "PorusPretrain(IDENTICAL)\n",
        "\n",
        "new, lgd = IDENTICAL.forward_transform(coor)\n",
        "flow.load_state_dict(IDENTICAL.state_dict())\n",
        "print(Identity(new, coor))\n",
        "torch.save(IDENTICAL.state_dict(), './data/IDENTICAL.pt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O6dA7f7lNFR7",
        "outputId": "706a833b-2531-4e20-94e6-bb7885de8a0c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/functional.py:445: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2157.)\n",
            "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(1.9304e-09, grad_fn=<MeanBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "abs = []\n",
        "rel = []\n",
        "\n",
        "for epoch in range(100):\n",
        "    t = 0.1 + 0.005*(epoch + 1)\n",
        "    optimizer = torch.optim.LBFGS(flow.parameters(), \n",
        "                            history_size = 80,\n",
        "                            max_iter = 80,\n",
        "                            line_search_fn= 'strong_wolfe')\n",
        "    def closure():\n",
        "        optimizer.zero_grad()\n",
        "        phi, lgdet = flow.forward_transform(coor)\n",
        "        lgdet.unsqueeze_(-1)\n",
        "        loss = Porus(phi, coor, rho, lgdet, alpha)[0]\n",
        "        loss.backward()\n",
        "        return loss\n",
        "\n",
        "    optimizer.step(closure)\n",
        "\n",
        "    phi, lgdet = flow.forward_transform(coor)\n",
        "    coor = phi\n",
        "    coor = coor.detach()\n",
        "    lgdet = lgdet.unsqueeze_(-1).detach()\n",
        "    rho = rho / (torch.exp(lgdet) + 1e-8)\n",
        "\n",
        "    torch.save(coor, f'./data/coor{epoch + 1}.pt')\n",
        "    torch.save(rho, f'./data/rho{epoch + 1}.pt')\n",
        "\n",
        "    act_val = Barenblatt(coor, t)\n",
        "    err = torch.mean((rho - act_val)**2)\n",
        "    relerr = err/torch.mean(act_val**2)\n",
        "\n",
        "    abs.append(err.item())\n",
        "    rel.append(relerr.item())\n",
        "\n",
        "\n",
        "    print(f'in epoch {epoch}, the L2 relative error is {relerr.item()}')\n",
        "    DensityPlot(coor.numpy()[:,0], coor.numpy()[:,1], rho.numpy(), epoch+1)\n",
        "\n",
        "    flow.load_state_dict(IDENTICAL.state_dict())"
      ],
      "metadata": {
        "id": "np-bzHNhNJoM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots(1, 1, figsize=(8, 5))\n",
        "# ax.set_title(r'$f = 2\\sin{x}\\cdot\\cos{y}$')\n",
        "interval = list(range(1,61))\n",
        "interval = [0.01*ele for ele in interval]\n",
        "# val = [abs(ele) for ele in ENERGY]\n",
        "# lines = []\n",
        "ax.set_yscale('log')\n",
        "lines = ax.plot(interval, rel[:60],  color= '#EE82EE' )\n",
        "ax.grid(color='grey', linestyle='-', linewidth=0.25, alpha=0.5)\n",
        "ax.set_xlabel('time', fontsize = 20)\n",
        "ax.set_ylabel('L2 Relative Eroor', fontsize = 20)\n",
        "ax.tick_params(axis='both', which='major', labelsize=20)\n",
        "plt.xlim([-0.01, 0.61])\n",
        "plt.savefig('./L2Relative.png',pad_inches = 0.05, bbox_inches='tight')\n",
        "plt.show()\n",
        "plt.close()"
      ],
      "metadata": {
        "id": "Trifhk5gRiTC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots(1, 1, figsize=(8, 5))\n",
        "# ax.set_title(r'$f = 2\\sin{x}\\cdot\\cos{y}$')\n",
        "interval = list(range(1,61))\n",
        "interval = [0.01*ele for ele in interval]\n",
        "# val = [abs(ele) for ele in ENERGY]\n",
        "# lines = []\n",
        "ax.set_yscale('log')\n",
        "lines = ax.plot(interval, abs[:60],  color= '#EE82EE' )\n",
        "ax.grid(color='grey', linestyle='-', linewidth=0.25, alpha=0.5)\n",
        "ax.set_xlabel('time', fontsize = 20)\n",
        "ax.set_ylabel('L2 Absolute Eroor', fontsize = 20)\n",
        "ax.tick_params(axis='both', which='major', labelsize=20)\n",
        "plt.xlim([-0.01, 0.61])\n",
        "plt.savefig('./L2Absolute.png',pad_inches = 0.05, bbox_inches='tight')\n",
        "plt.show()\n",
        "plt.close()"
      ],
      "metadata": {
        "id": "jJyYu2UsS0vx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(abs, './data/PorusAbsError.pt')\n",
        "torch.save(rel, './data/PorusRelError.pt')"
      ],
      "metadata": {
        "id": "bJAKkJVRcCSR"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r /content/plot.zip /content/plots/\n",
        "!zip -r /content/data.zip /content/data/"
      ],
      "metadata": {
        "id": "Q-LLDKJscQ4W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download(\"/content/data.zip\")\n",
        "files.download(\"/content/plot.zip\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "7lIUWx-md7Z2",
        "outputId": "35f36586-980f-43d1-83e5-5d6f0a90dcf9"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_98f8ac5a-a7ab-47ab-b22e-aad562cdedce\", \"data.zip\", 18261757)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_a2305c46-f3db-4f81-9393-d91d04049702\", \"plot.zip\", 46367790)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}